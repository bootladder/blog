<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2017-10-10T19:09:26-07:00</updated><id>/</id><title type="html">Bootladder News</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</subtitle><entry><title type="html">CMake and Gradle for ARM C/C++</title><link href="/2017/10/09/cmake-and-gradle-for-arm-c-c.html" rel="alternate" type="text/html" title="CMake and Gradle for ARM C/C++" /><published>2017-10-09T00:00:00-07:00</published><updated>2017-10-09T00:00:00-07:00</updated><id>/2017/10/09/cmake-and-gradle-for-arm-c-c</id><content type="html" xml:base="/2017/10/09/cmake-and-gradle-for-arm-c-c.html">&lt;p&gt;I’m trying to build my codebase in CMake or Gradle.&lt;br /&gt;
Tried Gradle first.  At first glance there are way too many DSL keywords,
I have no idea what they do and connecting the dots is insane.&lt;br /&gt;
It’s the same frustration I get with python.  Never knowing what type anything is
and always having to look up documentation before writing a single line.&lt;/p&gt;

&lt;p&gt;Now trying CMake…&lt;/p&gt;

&lt;p&gt;http://derekmolloy.ie/hello-world-introductions-to-cmake/&lt;/p&gt;

&lt;p&gt;I started with this CMakeLists.txt&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cmake_minimum_required(VERSION 2.8)
project(hello)
set(CMAKE_BINARY_DIR ${CMAKE_SOURCE_DIR}/bin)
set(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_DIR})
set(LIBRARY_OUTPUT_PATH ${CMAKE_BINARY_DIR})
include_directories(&quot;${PROJECT_SOURCE_DIR}&quot;)
add_executable(hello ${PROJECT_SOURCE_DIR}/src/main.c)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To build,&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cmake .
make
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Wow it actually tried to compile something.  No include directories were supplied.  Let’s add one.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include_directories(myincludedir)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Works.  Now there’s another header that’s not inside the repo.&lt;br /&gt;
Let’s add those.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include_directories(&quot;../samd20_cmsis_headers&quot;)
include_directories(&quot;../arm_cmsis_headers&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next error, Error: no such instruction: `cpsie i’.&lt;br /&gt;
I must not be compling with the arm gcc compiler.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set(CMAKE_C_COMPILER arm-none-eabi-gcc)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arm-none-eabi-gcc: error: unrecognized command line option '-rdynamic'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now I get this error.  Found a solution from here.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/digitalbitbox/mcu/blob/master/arm.cmake
# Avoid known bug in linux giving: 
#    arm-none-eabi-gcc: error: unrecognized command line option '-rdynamic'
set(CMAKE_SHARED_LIBRARY_LINK_C_FLAGS &quot;&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For the CFLAGS I used this syntax.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set(CMAKE_C_FLAGS &quot;-std=c99                      &quot;)
string(APPEND CMAKE_C_FLAGS &quot;-Wall                         &quot;)
string(APPEND CMAKE_C_FLAGS &quot;-Wextra                       &quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I used this to add directories containing source files to be compiled.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;file(GLOB SOURCES
    &quot;src/*.h&quot;
    &quot;src/*.c&quot;
    &quot;src/hal/*.c&quot;
    &quot;src/hal/*.h&quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;And I changed the executable to be built from those sources:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add_executable(myname ${SOURCES})
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I noticed the output of the GNU size command shows different sizes for the
ELF output from the CMake build and the original Makefile build.
  I realized I never specified the linker script.  Let’s do that.&lt;/p&gt;

&lt;p&gt;Turns out I just stick it in the CMAKE_C_FLAGS , no need for a separate linker flags.&lt;/p&gt;

&lt;p&gt;Added that and I get the desired output.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   text    data     bss     dec     hex filename     
  3352      68    2584    6004    1774 bin/debos_firmware  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Seeing that exact same output on either build is… extremely releiving.&lt;/p&gt;

&lt;h1 id=&quot;uh-now-im-trying-to-build-my-unit-test-executable&quot;&gt;Uh, now I’m trying to build my unit test executable.&lt;/h1&gt;

&lt;p&gt;I got this far …&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;file(GLOB TEST_SOURCES
    &quot;test/*.h&quot;
    &quot;test/*.c&quot;
    &quot;test/test_runners/*.c&quot;
    &quot;test/test_runners/*.h&quot;
    &quot;mock/*.c&quot;
    &quot;../Unity/src/*.c&quot;
    &quot;src/*.c&quot;
    &quot;src/hal/*.c&quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;First issue I have to deal with:&lt;br /&gt;
Multiple Definition error from the linker.&lt;br /&gt;
As you see, I included the test/ directory and src/.
Inside test there are some mocked objects.  The names collide with the ones from source.&lt;/p&gt;

&lt;p&gt;A solution I’ve learned from various books is to compile the source into a library.
When linking the test executable, the library will only be searched when the
symbol is not found in the test objects.&lt;br /&gt;
This hackish technique creates a priority of sorts, for names.  Test names first, production names second.&lt;/p&gt;

&lt;p&gt;Now how do I do this with CMake?  Reimplement that above logic?&lt;br /&gt;
My guess is Yes, because CMake just wraps make and it is make and gcc that are complaining
about not finding the symbols.&lt;/p&gt;

&lt;h1 id=&quot;create-a-static-library-of-the-application-code-and-link-it-to-the-test-exe&quot;&gt;Create a static library of the application code and link it to the test exe&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add_library(applib_debos_firmware STATIC ${SOURCES})
add_executable(test_debos_firmware ${TEST_SOURCES})
target_link_libraries(test_debos_firmware applib_debos_firmware )
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;First, the above CMakeLists.txt is inside the test/ directory.&lt;br /&gt;
So I have CMakeLists.txt inside both src/ and test/.&lt;br /&gt;
To build, I go inside a directory and run cmake, and the Makefile is generated in the directory.&lt;br /&gt;
Since the Makefile is now autogenerated, I now have it in .gitignore.&lt;/p&gt;

&lt;p&gt;The ${SOURCES} is a collection of source inside src/ , but does not include HAL.&lt;br /&gt;
HAL code is generally not compilable on host.&lt;br /&gt;
If there is a high level HAL like a UART driver then I’ll put that in src/ not hal/.&lt;/p&gt;

&lt;h1 id=&quot;so-far-this-is-good--next-i-want-the-dependencies-pulled-in-automatically&quot;&gt;So far this is good.  Next I want the dependencies pulled in automatically.&lt;/h1&gt;

&lt;p&gt;This particular project needs libraries:  unit test library, 
target specific headers repositories.&lt;/p&gt;

&lt;p&gt;Unit Test library is public URL, target specific headers are public git URL.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ExternalProject_Add(samd20_headers
    GIT_REPOSITORY &quot;https://bitbucket.org/bootladder/samd20_cmsis_headers&quot;
    BUILD_COMMAND &quot;&quot;
    UPDATE_COMMAND &quot;&quot;
    CONFIGURE_COMMAND &quot;&quot;
    INSTALL_COMMAND &quot;&quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This worked to get the repo downloaded.  The following showed up in my ls:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;drwxr-xr-x 4 steve supervisorusers 4.0K Oct 10 15:50 samd20_headers-prefix/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;What is this prefix business…  let’s put in a prefix.&lt;/p&gt;

&lt;p&gt;If I put in a&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PREFIX &quot;hello&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Then the repo is cloned, named hello.  what??&lt;br /&gt;
Apparently it’s supposed to be this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PREFIX &quot;samd20_headers&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Why do I have to explicitly type the same name…&lt;/p&gt;

&lt;h1 id=&quot;anyway-next-problem-is-the-include-path-is-no-longer-the-same&quot;&gt;Anyway, next problem is, the include path is no longer the same.&lt;/h1&gt;

&lt;p&gt;I need to tell CMake to add include directories to the compiler command.&lt;/p&gt;

&lt;h1 id=&quot;oh-prefix-is-for-where-the-external-project-is-installed--doesnt-have-to-be-inside-this-project&quot;&gt;Oh, prefix is for where the external project is installed.  Doesn’t have to be inside this project&lt;/h1&gt;

&lt;h1 id=&quot;moving-on-to-the-test-build-auto-downloading-the-unity-test-framework&quot;&gt;Moving on to the test build, auto downloading the Unity test framework&lt;/h1&gt;

&lt;p&gt;This one was tricky because it doesn’t provide a build script.
It is intended for the library consumer to build the source.&lt;br /&gt;
Also, the source is not in the top directory of the repo.&lt;/p&gt;

&lt;p&gt;I solved it with a kludge, don’t like it:&lt;/p&gt;

&lt;p&gt;First, this is how I added compiler include directories.&lt;br /&gt;
Using install_dir as a global variable here…&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ExternalProject_Get_Property(samd20_headers install_dir)
include_directories(${install_dir}/src/samd20_headers/)
ExternalProject_Get_Property(arm_cmsis_headers install_dir)
include_directories(${install_dir}/src/arm_cmsis_headers/)
ExternalProject_Get_Property(unity install_dir)
include_directories(${install_dir}/src/unity/src/)
include_directories(${install_dir}/src/unity/extras/fixture/src/)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice at the end, for unity, there are 2 calls to include_directories().
I guess it’s ok, it’s explicit and still relative to the ${install_dir}.&lt;/p&gt;

&lt;p&gt;Later, in my list of source files, there’s another kludge.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;file(GLOB TEST_SOURCES
    &quot;*.h&quot;
    &quot;*.c&quot;
    &quot;hal/*.c&quot;
    &quot;test_runners/*.c&quot;
    &quot;test_runners/*.h&quot;
    &quot;../../Unity/extras/fixture/src/*.c&quot;
    &quot;../mock/*.c&quot;
    &quot;../../Unity/src/*.c&quot;
    &quot;${install_dir}/src/unity/src/*&quot;
    &quot;${install_dir}/src/unity/extras/fixture/src/*&quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;But same, I guess it’s OK, because the list is explicit and also relative to ${install_dir}.&lt;br /&gt;
So every time this project is cloned and built it’ll work.&lt;br /&gt;
One issue is the name install_dir is only valid when install_dir was just written to with the correct contents.&lt;/p&gt;

&lt;h1 id=&quot;uhhh-a-similar-issue-build-doesnt-work-first-time-works-second-time&quot;&gt;Uhhh, a similar issue… Build doesn’t work first time, works second time?&lt;/h1&gt;

&lt;p&gt;What’s happening here…&lt;/p&gt;

&lt;p&gt;Same thing as before.  The Unity git repo was cloned, and then
Make is supposed to compile the source as part of the build.&lt;br /&gt;
But Make can’t find them.&lt;br /&gt;
In this case, Make did not include those sources in the list of source files to be compiled.&lt;br /&gt;
Make said “Scanning dependencies of…” and then compiled the sources, but the Unity sources weren’t.&lt;br /&gt;
All I have to do to get the build to succeed next time is:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;touch CMakeLists.txt
make
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;And it works; this time, it scans the dependencies, finds the Unity sources and builds them.&lt;/p&gt;

&lt;h1 id=&quot;i-guess-im-not-supposed-to-try-to-do-this&quot;&gt;I guess I’m not supposed to try to do this?&lt;/h1&gt;

&lt;p&gt;Why should I build the source ?  That means I have to know how to build it.&lt;br /&gt;
Any changes to that repo will break my build and everyone else who does it this way.&lt;/p&gt;

&lt;p&gt;What’s the alternative?  They can’t precompile unless they only supported a couple targets.&lt;br /&gt;
They could include a Makefile or CMakeLists.txt.  CMake would work I guess but not Make.&lt;/p&gt;

&lt;h1 id=&quot;lets-follow-this-one--httpwwwthrowtheswitchorgbuildcmake&quot;&gt;Let’s follow this one:  http://www.throwtheswitch.org/build/cmake&lt;/h1&gt;

&lt;p&gt;Since it is literally what I’m trying to do.&lt;/p&gt;

&lt;h1 id=&quot;eh-lets-get-ceedling-working-first&quot;&gt;Eh, let’s get Ceedling working first.&lt;/h1&gt;

&lt;p&gt;Currently I had handwritten Unity tests.&lt;br /&gt;
Ceedling does it a bit different.&lt;br /&gt;
Each test.c file is linked into a test app and executed.&lt;br /&gt;
It uses its own build system so anything that was in Make or CMake before
is invalid.&lt;br /&gt;
This means the linker and header dependencies on the production source.&lt;/p&gt;

&lt;p&gt;To get the ceedling tests to work you have to put #include statements
in the tests, which makes Ceedling pull in source files.&lt;br /&gt;
Other than that seems OK.&lt;/p&gt;

&lt;h1 id=&quot;problem-is-you-need-a-ruby-and-ceedling-installation&quot;&gt;Problem is you need a ruby and ceedling installation.&lt;/h1&gt;</content><author><name></name></author><summary type="html">I’m trying to build my codebase in CMake or Gradle. Tried Gradle first. At first glance there are way too many DSL keywords, I have no idea what they do and connecting the dots is insane. It’s the same frustration I get with python. Never knowing what type anything is and always having to look up documentation before writing a single line. Now trying CMake… http://derekmolloy.ie/hello-world-introductions-to-cmake/ I started with this CMakeLists.txt cmake_minimum_required(VERSION 2.8) project(hello) set(CMAKE_BINARY_DIR ${CMAKE_SOURCE_DIR}/bin) set(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_DIR}) set(LIBRARY_OUTPUT_PATH ${CMAKE_BINARY_DIR}) include_directories(&quot;${PROJECT_SOURCE_DIR}&quot;) add_executable(hello ${PROJECT_SOURCE_DIR}/src/main.c) To build, cmake . make Wow it actually tried to compile something. No include directories were supplied. Let’s add one. include_directories(myincludedir) Works. Now there’s another header that’s not inside the repo. Let’s add those. include_directories(&quot;../samd20_cmsis_headers&quot;) include_directories(&quot;../arm_cmsis_headers&quot;)</summary></entry><entry><title type="html">Reverse SSH access to beaglebone</title><link href="/2017/10/05/reverse-ssh-access-to-beaglebone.html" rel="alternate" type="text/html" title="Reverse SSH access to beaglebone" /><published>2017-10-05T00:00:00-07:00</published><updated>2017-10-05T00:00:00-07:00</updated><id>/2017/10/05/reverse-ssh-access-to-beaglebone</id><content type="html" xml:base="/2017/10/05/reverse-ssh-access-to-beaglebone.html">&lt;p&gt;I’m leaving for the weekend and I want access to my 2 beaglebones.&lt;br /&gt;
1 is my system under test, the other controls a USB programmer
that can flash a microcontroller on the SUT.&lt;/p&gt;

&lt;p&gt;Following this guide http://xmodulo.com/access-linux-server-behind-nat-reverse-ssh-tunnel.html&lt;/p&gt;

&lt;p&gt;First I’ll locally SSH into the beaglebone.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;homeserver~$ ssh -fN -R 10022:localhost:22 relayserver_user@1.1.1.1 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;-R for reverse tunnel.  Port 10022 on remote host is forwarded to port 22 on beaglebone&lt;/li&gt;
  &lt;li&gt;10022 is arbitrary&lt;/li&gt;
  &lt;li&gt;port 22 is the SSH port that beaglebone sshd is listening on&lt;/li&gt;
  &lt;li&gt;-f for background&lt;/li&gt;
  &lt;li&gt;-N for “don’t execute a command”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:  I forgot that my VPS had disabled password logins; only permitted by key.&lt;br /&gt;
I had only installed 1 key on the VPS:  for my main laptop.&lt;br /&gt;
So to install 2 more keys, I SSHed into the VPS with my main laptop.&lt;br /&gt;
Temporarily allowed password logins.&lt;br /&gt;
Then used ssh-copy-id to install the keys on VPS.&lt;br /&gt;
Finally, disable password logins again.&lt;/p&gt;

&lt;p&gt;Last step is to add the command in rc.local.&lt;/p&gt;

&lt;h1 id=&quot;couple-extra-details&quot;&gt;Couple extra details&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;One of the beaglebones was running SSH on port 6000.  So, I had to change the 22 to a 6000.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;homeserver~$ ssh -fN -R 10022:localhost:6000 relayserver_user@1.1.1.1 
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;One of the beaglebones was only accepting SSH logins by key, but it only accepted one of a static set of keys.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was annoying, I had to scp the key over to the VPS.  Then to login to the beaglebone thru the tunnel,
I had to supply the key with ssh -i.&lt;/p&gt;

&lt;h1 id=&quot;important-detail-for-having-2-tunnels&quot;&gt;Important detail for having 2+ tunnels&lt;/h1&gt;

&lt;p&gt;The port 10022 is arbitrary but if there are 2 tunnels they can’t be both on 10022.&lt;br /&gt;
So actually one of them was on 10023.&lt;br /&gt;
When SSHing through the tunnel I specified which tunnel by specifying the port.&lt;/p&gt;

&lt;h1 id=&quot;dang-the-ssh-connection-disconnected-overnight&quot;&gt;Dang, the SSH connection disconnected overnight.&lt;/h1&gt;

&lt;p&gt;Using AutoSSH 
https://www.everythingcli.org/ssh-tunnelling-for-fun-and-profit-autossh/&lt;/p&gt;

&lt;p&gt;Looks promising, I just used standard config.&lt;br /&gt;
Let’s see if it stays up!&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m leaving for the weekend and I want access to my 2 beaglebones. 1 is my system under test, the other controls a USB programmer that can flash a microcontroller on the SUT. Following this guide http://xmodulo.com/access-linux-server-behind-nat-reverse-ssh-tunnel.html First I’ll locally SSH into the beaglebone.</summary></entry><entry><title type="html">Keep my hosts running</title><link href="/2017/10/04/keep-my-hosts-running.html" rel="alternate" type="text/html" title="Keep my hosts running" /><published>2017-10-04T00:00:00-07:00</published><updated>2017-10-04T00:00:00-07:00</updated><id>/2017/10/04/keep-my-hosts-running</id><content type="html" xml:base="/2017/10/04/keep-my-hosts-running.html">&lt;p&gt;Now that I temporarily host my friend’s site who actually wants to have visitors,
I can’t let it go down.  Also this blog, might as well keep it up.&lt;/p&gt;

&lt;p&gt;So first, when the VPS host goes down and reboots my VPS, my VPS has to start up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reverse Proxy, blog, friend’s site&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;systemd service looks OK but… feel safer with a shell script.&lt;/p&gt;

&lt;p&gt;To get this going, I’ll reboot the VPS and repeat the steps.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;service docker start;
cd /opt/nginx-proxy/ ; docker-compose up &amp;amp;
cd /opt/deploy/ ; docker-compose up &amp;amp;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;stick it in rc.local&lt;/p&gt;

&lt;p&gt;nice!&lt;/p&gt;

&lt;p&gt;Now how about a way to check if the website ever goes down?&lt;br /&gt;
Well if the host goes down it can’t tell me about it.&lt;br /&gt;
So, it could be the stopping of keepalive messages out of the host,
or a different host being unable to connect to the host.&lt;/p&gt;

&lt;p&gt;eh, leave that one at that for now.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;l&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Now that I temporarily host my friend’s site who actually wants to have visitors, I can’t let it go down. Also this blog, might as well keep it up. So first, when the VPS host goes down and reboots my VPS, my VPS has to start up.</summary></entry><entry><title type="html">beaglebone MCU programmer unit</title><link href="/2017/10/04/beaglebone-mcu-programmer-unit.html" rel="alternate" type="text/html" title="beaglebone MCU programmer unit" /><published>2017-10-04T00:00:00-07:00</published><updated>2017-10-04T00:00:00-07:00</updated><id>/2017/10/04/beaglebone-mcu-programmer-unit</id><content type="html" xml:base="/2017/10/04/beaglebone-mcu-programmer-unit.html">&lt;p&gt;I’m flashing Atmel ARM microcontrollers with a USB programmer dongle.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/bootladder/edbg
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Got a fresh beaglebone running, grabbed the IP from the local router.&lt;br /&gt;
In order to get the git clone to work I had to get the time right.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install ntp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;the compiled edbg binary is not included so it must be installed.&lt;br /&gt;
Let’s write the install script here.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp libudev.h /usr/include;
cp libudev.so /lib ;
make;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hmm looks like the library I included in there is out of date.  libc perhaps?&lt;/p&gt;

&lt;p&gt;Fortunately it worked after a&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libudev-dev 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So I’ll just take that copy of libudev.so out of the repo.&lt;/p&gt;

&lt;p&gt;So now it looks like this&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libudev-dev
make;
cp edbg /bin/;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then I run a&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@beaglebone:/tmp# edbg -l
Attached debuggers:
  J41800059345 - Atmel Corp. Atmel-ICE CMSIS-DAP
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Sweet!&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m flashing Atmel ARM microcontrollers with a USB programmer dongle. https://github.com/bootladder/edbg Got a fresh beaglebone running, grabbed the IP from the local router. In order to get the git clone to work I had to get the time right. apt-get install ntp the compiled edbg binary is not included so it must be installed. Let’s write the install script here. cp libudev.h /usr/include; cp libudev.so /lib ; make; Hmm looks like the library I included in there is out of date. libc perhaps? Fortunately it worked after a apt-get install libudev-dev So I’ll just take that copy of libudev.so out of the repo. So now it looks like this apt-get install libudev-dev make; cp edbg /bin/; Then I run a root@beaglebone:/tmp# edbg -l Attached debuggers: J41800059345 - Atmel Corp. Atmel-ICE CMSIS-DAP Sweet!</summary></entry><entry><title type="html">Use Phone WiFi Hotspot as default gateway for LAN</title><link href="/2017/09/18/use-phone-wifi-hotspot-as-default-gateway-for-lan.html" rel="alternate" type="text/html" title="Use Phone WiFi Hotspot as default gateway for LAN" /><published>2017-09-18T00:00:00-07:00</published><updated>2017-09-18T00:00:00-07:00</updated><id>/2017/09/18/use-phone-wifi-hotspot-as-default-gateway-for-lan</id><content type="html" xml:base="/2017/09/18/use-phone-wifi-hotspot-as-default-gateway-for-lan.html">&lt;p&gt;Big telecom companies are garbage.  Turd sandwich or giant douche?&lt;br /&gt;
Comcast: 1 year contract for $100/mo, 50Mb down.  No thanks!&lt;br /&gt;
Let’s look into mobile internet with unlimited data.  Let’s see if I can do this with my phone.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First I grab an extra laptop that has both Ethernet and WiFi.  This is the gateway&lt;/li&gt;
  &lt;li&gt;I turn on my phone’s hotspot and connect to it with the gateway laptop&lt;/li&gt;
  &lt;li&gt;Verify I can ping the gateway laptop from my tester client&lt;/li&gt;
  &lt;li&gt;Change default gateway on tester client to 10.0.0.6, verify Internet is now unavailable&lt;/li&gt;
  &lt;li&gt;On the gateway laptop, run this&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo sysctl -w net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now Internet is available through gateway laptop, but the gateway now has to go out the WiFi hotspot.&lt;/p&gt;

&lt;p&gt;Turns out there are 2 0.0.0.0 entries in the route table.  Let’s delete the one for Ethernet and keep the WiFi one.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;route del default gateway x.x.x.x
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hmm, ping doesn’t work.  Just hangs, no ouptut.  I see what’s happening using tcpdump … traffic starts at the tester client, gets to the gateway on the Ethernet interface, then goes out the WiFi interface, but the source address is still the tester client’s source.&lt;br /&gt;
If I run a ping on the gateway laptop, it does go through.  The obvious difference is the source address.  Let’s change that.&lt;/p&gt;

&lt;p&gt;Let’s try this one&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# iptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j SNAT --to-source 1.2.3.4
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;wow-that-made-it-work&quot;&gt;Wow, that made it work!&lt;/h1&gt;</content><author><name></name></author><summary type="html">Big telecom companies are garbage. Turd sandwich or giant douche? Comcast: 1 year contract for $100/mo, 50Mb down. No thanks! Let’s look into mobile internet with unlimited data. Let’s see if I can do this with my phone. First I grab an extra laptop that has both Ethernet and WiFi. This is the gateway I turn on my phone’s hotspot and connect to it with the gateway laptop Verify I can ping the gateway laptop from my tester client Change default gateway on tester client to 10.0.0.6, verify Internet is now unavailable On the gateway laptop, run this sudo sysctl -w net.ipv4.ip_forward=1 Now Internet is available through gateway laptop, but the gateway now has to go out the WiFi hotspot. Turns out there are 2 0.0.0.0 entries in the route table. Let’s delete the one for Ethernet and keep the WiFi one. route del default gateway x.x.x.x Hmm, ping doesn’t work. Just hangs, no ouptut. I see what’s happening using tcpdump … traffic starts at the tester client, gets to the gateway on the Ethernet interface, then goes out the WiFi interface, but the source address is still the tester client’s source. If I run a ping on the gateway laptop, it does go through. The obvious difference is the source address. Let’s change that. Let’s try this one # iptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j SNAT --to-source 1.2.3.4 Wow, that made it work!</summary></entry><entry><title type="html">Making my version control not suck</title><link href="/2017/09/13/making-my-version-control-not-suck.html" rel="alternate" type="text/html" title="Making my version control not suck" /><published>2017-09-13T00:00:00-07:00</published><updated>2017-09-13T00:00:00-07:00</updated><id>/2017/09/13/making-my-version-control-not-suck</id><content type="html" xml:base="/2017/09/13/making-my-version-control-not-suck.html">&lt;p&gt;I develop and maintain firmware for a family of products.
There are 5 different firmware projects here.
They’re all releated in communication protocol, as well as other things
such as beep frequency, LED blink patterns etc.&lt;/p&gt;

&lt;p&gt;But now I’m faced with splitting all 5 of those into 3 versions.
So now there are technically 15 different firmware images that need to be
available for download and documented to identify what they are.&lt;/p&gt;

&lt;p&gt;My (git) version control system until now was obvious.  Master goes forward, occasional branching and merging, tags on version releases.&lt;/p&gt;

&lt;p&gt;It would be possible to use branches and tags to rebuild any version of any project.  But here’s the complication:  there is a in-house shared library used by all 15 firmware versions.  That library also has 3 versions.  In other words the correct version of the shared library has to be available when building a version of firmware.
  In more other words, to build a version of firmware, the project repo has to be checked out at the right version, as well as the shared library checked out at the right version.  Finally, the build config (eg. Makefile) has to identify and locate the correct shared library to be linked.&lt;/p&gt;

&lt;p&gt;I like stuff like Ruby, Rust, etc. where you have a config file (yaml) to specify dependencies.&lt;/p&gt;

&lt;p&gt;I think I can improve my Makefile to do what I want.&lt;br /&gt;
It would be as simple as git checkout my_commit followed by make.
The Makefile at this commit should build the proper version.  The Makefile itself identifies its own version and the dependency versions it needs.  It can then git clone the dependencies using git submodules.&lt;/p&gt;

&lt;p&gt;This way, a complete build with all dependencies lives inside 1 directory.  If a submodule is updated or branched, it won’t affect previous builds.&lt;/p&gt;

&lt;h1 id=&quot;strategy--use-a-fresh-machine-vps-to-recreate-the-build&quot;&gt;Strategy:  use a fresh machine (VPS) to recreate the build&lt;/h1&gt;

&lt;p&gt;I don’t want to be confused with currently existing directory structure.  Building the same firmware version on a different machine is a good way to shake out things like hard coded paths breaking.&lt;br /&gt;
Ideally all builds would happen on this separate machine.  I guess that’s part of Continous Integration?&lt;/p&gt;

&lt;h1 id=&quot;-makefile-needs-source-located-in-a-hard-coded-directory&quot;&gt;— Makefile needs source located in a hard coded directory&lt;/h1&gt;

&lt;p&gt;Project needs a library or some source somewhere else to compile.
The Makefile can find the source with a VPATH and find the headers
with -I compiler flags.  But, that enforces a certain directory structure.&lt;/p&gt;

&lt;h1 id=&quot;-new-rule-discovery-only-create-directory-structure-for-sub-directories--never-depend-on-paths-starting-wtih--or--etc&quot;&gt;— New rule discovery?: Only create directory structure for sub-directories.  Never depend on paths starting wtih ../ or ../../ etc.&lt;/h1&gt;

&lt;p&gt;This rule allows you to create directory structure, but doesn’t cause problems because the structure is all below current directory.&lt;br /&gt;
Basically, enforced sub-directory structure is coupled entirely to
the populating of that sub-directory and makes no assumptions about directories above current directory.&lt;br /&gt;
In other words, if you have enforced structure, the build system has to automatically conform to that structure.  Let the Makefile create the directories with their correct names, populate them with files.&lt;/p&gt;

&lt;h1 id=&quot;-vpath-issue-with-git-clone&quot;&gt;— VPATH Issue with git clone&lt;/h1&gt;

&lt;p&gt;I have a repo prereq inside my all target.
The purpose is to clone the dependencies,
so Make can then refer to those dependencies and build the target.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;always: 
repos: always
        @echo hello
        if [ ! -d &quot;../../mysrcrepo&quot; ] ; then \
            git clone https://mysrcrepo ;\
        fi 
        if [ ! -d &quot;../../mylibrepo&quot; ] ; then \
            git clone https://mylibrepo ;\
            make; \

        fi 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It almost worked.
  In the case of a cloned library, the make command is run, which builds the library.  That worked.
  But in the case of cloned source, the VPATH didn’t catch the source files that were cloned.  When running make a second time, it would work.&lt;/p&gt;

&lt;p&gt;Well apparently 2 things are wrong with that, as I’ve read.&lt;br /&gt;
1, VPATH doesn’t work on generated files.  Not sure what that means specifically but the way I made sense of it is this:  You can add directories to VPATH but if they don’t exist when make is run, the VPATH won’t see that directory.  Even if the file exists at the time it is needed, it won’t be found.  That explains why the build works the second time.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Recursive Make Considered Harmful?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;discovery--how-to-do-git-inside-make&quot;&gt;Discovery:  how to do git inside make&lt;/h1&gt;

&lt;p&gt;https://stackoverflow.com/questions/15602059/git-shortcut-to-pull-with-clone-if-no-local-there-yet&lt;/p&gt;

&lt;p&gt;Using this I can solve the problem in the above snippet
        if [ ! -d “../../mysrcrepo” ] ; then \&lt;/p&gt;

&lt;p&gt;If the directory does exist, nothing happens.  What I want is a git pull in that case.&lt;/p&gt;</content><author><name></name></author><summary type="html">I develop and maintain firmware for a family of products. There are 5 different firmware projects here. They’re all releated in communication protocol, as well as other things such as beep frequency, LED blink patterns etc.</summary></entry><entry><title type="html">super fast static website migration</title><link href="/2017/09/08/super-fast-static-website-migration.html" rel="alternate" type="text/html" title="super fast static website migration" /><published>2017-09-08T00:00:00-07:00</published><updated>2017-09-08T00:00:00-07:00</updated><id>/2017/09/08/super-fast-static-website-migration</id><content type="html" xml:base="/2017/09/08/super-fast-static-website-migration.html">&lt;p&gt;A friend was paying a company a crazy amount of money per month to create a static site,
and host it.&lt;br /&gt;
Friend realized they were being ripped off and stopped paying.
The company then shutdown the website host, blocking access to the site source.&lt;/p&gt;

&lt;p&gt;Fortunately there was a copy on the wayback machine.&lt;/p&gt;

&lt;p&gt;I ripped the last good copy and dumped it onto my own host.&lt;/p&gt;

&lt;p&gt;It was super easy and fast, took just a few hours to figure out a couple things.&lt;/p&gt;

&lt;p&gt;The entire website was roughly 1MB.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Actually you don’t even need a Dockerfile, you can work straight from this image
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  from sebp/lighttpd
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;docker-compose.yaml
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  version: '2'
  services:
    lighttpd:
      build: .
      environment:
        - VIRTUAL_HOST=mywebsite.com
      ports:
        - &quot;14001:80&quot;
      volumes:
        - ./website/:/var/www/localhost/htdocs

  networks:
    default:
      external:
        name: nginx-proxy
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;explanation&quot;&gt;Explanation:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;the environment variable VIRTUAL_HOST is for the nginx reverse proxy.&lt;br /&gt;
It’s name based so when you HTTP request the reverse proxy looking for hostname
mywebsite.com , the reverse proxy directs traffic to this container we’re discussing.&lt;/li&gt;
  &lt;li&gt;ports: the container lighttpd serves on port 80.  14001 is a random number.&lt;/li&gt;
  &lt;li&gt;volumes: ./website , ie. the current directory where the docker-compose.yaml file is,
is mapped to the web root of the lighttpd server.
Just stick the website in there.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;to-grab-the-website-from-wayback-machine-i-used-wayback-machine-downloader&quot;&gt;To grab the website from Wayback Machine, I used Wayback Machine Downloader&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/hartator/wayback-machine-downloader&quot;&gt;https://github.com/hartator/wayback-machine-downloader&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Was super easy to use, just took a minute to figure out the –to and –from flags.&lt;/li&gt;
  &lt;li&gt;When the site you’re interested in is currently pointing to a domain parking spot,
you want to get files up to a certain date.&lt;/li&gt;
  &lt;li&gt;If you want to exclude older files you’ll have to do that explicitly also.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">A friend was paying a company a crazy amount of money per month to create a static site, and host it. Friend realized they were being ripped off and stopped paying. The company then shutdown the website host, blocking access to the site source. Fortunately there was a copy on the wayback machine. I ripped the last good copy and dumped it onto my own host. It was super easy and fast, took just a few hours to figure out a couple things. The entire website was roughly 1MB.</summary></entry><entry><title type="html">Deploy blog updates automatically with git push</title><link href="/guide/2017/08/30/automatic-deploy-this-blog.html" rel="alternate" type="text/html" title="Deploy blog updates automatically with git push" /><published>2017-08-30T00:00:00-07:00</published><updated>2017-08-30T00:00:00-07:00</updated><id>/guide/2017/08/30/automatic-deploy-this-blog</id><content type="html" xml:base="/guide/2017/08/30/automatic-deploy-this-blog.html">&lt;p&gt;After I had the docker shared volume + git solution working (see other post),
I immediately noticed there were too many steps to work on the blog.&lt;/p&gt;

&lt;p&gt;Change blog source –&amp;gt; jekyll build –&amp;gt; git add &amp;amp;&amp;amp; git commit &amp;amp;&amp;amp; git push –&amp;gt; git pull&lt;/p&gt;

&lt;p&gt;The worst part being that git pull happens on the production host and the others happen on dev host.&lt;/p&gt;

&lt;p&gt;So, I found a bunch of posts talking about a push-to-deploy solution using git –bare.
It works by pushing commits to a bare repository located on production host.
The bare repo then has a post receive hook which installs files somewhere else on the production filesytem.&lt;/p&gt;

&lt;p&gt;I want to skip a step there, because fortunately, the Docker setup for this blog
maps the web server root directory to inside the git repo.  So no separate install directory needed.&lt;/p&gt;

&lt;p&gt;This guide is relevant, actually obvious but it’s nice to see it in a guide.  https://gist.github.com/joahking/780877&lt;/p&gt;

&lt;p&gt;Trying it out now…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stop the running Docker container&lt;/li&gt;
  &lt;li&gt;Go into a new directory so we don’t touch the working install&lt;/li&gt;
  &lt;li&gt;git clone –bare https://myrepo&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I get a blog.git directory, no source.
Ahh, now I get it.  There’s no working tree, there’s only objects (blobs).  The raw git stuff.
A working tree at any commit can be constructed using the objects and the history (did I say that right?).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;So, I can’t just have a bare repo.  I can’t have a non-bare repo because I can’t push to it.&lt;/li&gt;
  &lt;li&gt;So I guess the other guys were right.&lt;br /&gt;
Push to the bare repo and use the post receive hook
to create a working copy and install that.&lt;/li&gt;
  &lt;li&gt;But that doesn’t work if my Docker container is already running&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If Docker container is running, that means docker-compose up happened inside a git repo. (in my case)
If there was a separate bare repo, its post receive hook would install files inside another git repo.
Which is not too bad since I won’t ever push from that repo.
Now consider if the Dockerfile changes.  The container has to be rebuilt and restarted.
Pulling into the production non-bare repo might be awkward since its data is behind master in the history.
The post receive hook could copy the Dockerfile.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This may be more complicated than I like.  But perhaps the proper solution is best.
On production host we have the bare repo.&lt;br /&gt;
On dev we push to production –&amp;gt;  bare repo executes post receive
That post receive will install the Dockerfile, docker-compose.yaml, and the static website
into another location.&lt;br /&gt;
Actually this is nice since docker-compose.yaml uses relative paths “./” when mapping the
volume for http server root.&lt;br /&gt;
So we can actually deploy the website in any directory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Nice! It worked as expected.&lt;/li&gt;
  &lt;li&gt;git remote add prod&lt;/li&gt;
  &lt;li&gt;git push prod master&lt;/li&gt;
  &lt;li&gt;the bare prod repo executes its script and files appear inside the deploy directory!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I used the post-receive script from here
https://gist.github.com/thomasfr/9691385&lt;/p&gt;

&lt;p&gt;Works great.&lt;/p&gt;

&lt;p&gt;Actually you know what’s really cool, I don’t even need github.com to do this!  It’s just my laptop and prod.&lt;/p&gt;

&lt;p&gt;I could actually automate this further and combine:  jekyll build –&amp;gt; git add . –&amp;gt; git commit -m “insert message here” –&amp;gt; git push 
Into 1 command.&lt;/p&gt;

&lt;h1 id=&quot;now-i-can-deploy-changes-to-my-blog-with-1-git-push--as-demonstrated-by-this-particular-change&quot;&gt;Now I can deploy changes to my blog with 1 git push!  As demonstrated by this particular change!&lt;/h1&gt;</content><author><name></name></author><summary type="html">After I had the docker shared volume + git solution working (see other post), I immediately noticed there were too many steps to work on the blog. Change blog source –&amp;gt; jekyll build –&amp;gt; git add &amp;amp;&amp;amp; git commit &amp;amp;&amp;amp; git push –&amp;gt; git pull The worst part being that git pull happens on the production host and the others happen on dev host. So, I found a bunch of posts talking about a push-to-deploy solution using git –bare. It works by pushing commits to a bare repository located on production host. The bare repo then has a post receive hook which installs files somewhere else on the production filesytem. I want to skip a step there, because fortunately, the Docker setup for this blog maps the web server root directory to inside the git repo. So no separate install directory needed. This guide is relevant, actually obvious but it’s nice to see it in a guide. https://gist.github.com/joahking/780877 Trying it out now… Stop the running Docker container Go into a new directory so we don’t touch the working install git clone –bare https://myrepo I get a blog.git directory, no source. Ahh, now I get it. There’s no working tree, there’s only objects (blobs). The raw git stuff. A working tree at any commit can be constructed using the objects and the history (did I say that right?). So, I can’t just have a bare repo. I can’t have a non-bare repo because I can’t push to it. So I guess the other guys were right. Push to the bare repo and use the post receive hook to create a working copy and install that. But that doesn’t work if my Docker container is already running If Docker container is running, that means docker-compose up happened inside a git repo. (in my case) If there was a separate bare repo, its post receive hook would install files inside another git repo. Which is not too bad since I won’t ever push from that repo. Now consider if the Dockerfile changes. The container has to be rebuilt and restarted. Pulling into the production non-bare repo might be awkward since its data is behind master in the history. The post receive hook could copy the Dockerfile. This may be more complicated than I like. But perhaps the proper solution is best. On production host we have the bare repo. On dev we push to production –&amp;gt; bare repo executes post receive That post receive will install the Dockerfile, docker-compose.yaml, and the static website into another location. Actually this is nice since docker-compose.yaml uses relative paths “./” when mapping the volume for http server root. So we can actually deploy the website in any directory. Nice! It worked as expected. git remote add prod git push prod master the bare prod repo executes its script and files appear inside the deploy directory!</summary></entry><entry><title type="html">Hosting on docker behind a reverse proxy</title><link href="/guide/2017/08/29/docker-reverse-proxy-hosting.html" rel="alternate" type="text/html" title="Hosting on docker behind a reverse proxy" /><published>2017-08-29T00:00:00-07:00</published><updated>2017-08-29T00:00:00-07:00</updated><id>/guide/2017/08/29/docker-reverse-proxy-hosting</id><content type="html" xml:base="/guide/2017/08/29/docker-reverse-proxy-hosting.html">&lt;p&gt;Needed to redo my personal infrastructure.  My infrastructure at work absolutely needs work
but that’s out of my control.  But my personal stuff can complement my work stuff.&lt;/p&gt;

&lt;p&gt;Currently thinking about integrating:  blog, wiki, VPN, storage, CI server, etc.
And I would like some resources, particularly storage to be hostable on physical hardware in a home/office LAN.&lt;/p&gt;

&lt;p&gt;And in some way that isn’t a pile of mess and I can actually see what I have and what’s happening.&lt;/p&gt;

&lt;p&gt;First thing I did was point all my DNS at a reverse proxy server.&lt;br /&gt;
I wanted the RPS to run in a docker container.&lt;br /&gt;
First attempted to install docker on my $12/year VPS.  Turns out it was a OpenVZ VPS which did not have the minimum kernel 3.10 for docker.&lt;br /&gt;
So had to get a $15/year KVM VPS.  Cheap but only 5G disk.  Makes sense to be primarly a proxy and possibly the VPN server; not sure about VPN speed on a VPS…&lt;/p&gt;

&lt;p&gt;Used jwilder/nginx-proxy on docker hub.
Works great, also can add your own config for a host.  For example I want some hosts behind the proxy to be external to the proxy machine, ie. not running in a container inside the proxy machine.&lt;/p&gt;

&lt;p&gt;Then I wanted to host this blog behind the proxy.
lighttpd install.  This particular one has a VOLUME of its data directory.
So I just use docker-compose.yaml to map the volume to where the static site is stored.&lt;/p&gt;

&lt;p&gt;I tried putting my wiki behind the proxy but was getting weird behavior most probably related to URLs or HTTP headers.&lt;br /&gt;
Actually it mostly worked but logging in made it act weird.&lt;/p&gt;

&lt;p&gt;Now that I could host the site in a docker container and have it behind the proxy,
I had to figure how to deploy changes.&lt;br /&gt;
Found a solution, though it has coupling not sure how to avoid… chef/puppet?&lt;/p&gt;

&lt;p&gt;I create a git repository containing:
the static site source code, and the built site.
a docker-compose.yaml and Dockerfile&lt;/p&gt;

&lt;p&gt;to develop I have a laptop that can build jekyll, so I pull the repo, edit/new files, 
jekyll build, push the repo which includes the built site.&lt;/p&gt;

&lt;p&gt;Then on my production host I run git pull to pull the built site.
Then docker-compose up serves the site.&lt;/p&gt;

&lt;p&gt;I don’t even have to touch docker!  I just go to production host where the repo is, git pull, and since the volume is mapped, the changes show immediately.&lt;/p&gt;

&lt;p&gt;There is coupling to the reverse proxy; a docker network.&lt;/p&gt;</content><author><name></name></author><summary type="html">Needed to redo my personal infrastructure. My infrastructure at work absolutely needs work but that’s out of my control. But my personal stuff can complement my work stuff.</summary></entry><entry><title type="html">How to test getopt</title><link href="/guide/2017/06/20/Unit-Testing-Legacy-C-Embedded.html" rel="alternate" type="text/html" title="How to test getopt" /><published>2017-06-20T00:00:00-07:00</published><updated>2017-06-20T00:00:00-07:00</updated><id>/guide/2017/06/20/Unit-Testing-Legacy-C-Embedded</id><content type="html" xml:base="/guide/2017/06/20/Unit-Testing-Legacy-C-Embedded.html">&lt;p&gt;I have a function with associated unit tests, called Parse_Bootloader_Commandline().
  int Parse_Bootloader_Commandline(int argc, char * const argv[])&lt;br /&gt;
It clearly takes in the argc and argv from main(), and parses it.&lt;/p&gt;

&lt;p&gt;I had it working, and then I wanted to change the internals of that function, to use getopt.&lt;br /&gt;
Interesting, I found that the unit tests were still applicable.  They had names eg.&lt;br /&gt;
ParseCommandline_InvalidCommand_Returns0&lt;br /&gt;
ParseCommandline_ValidLoadCommand_SetsCommandVariable&lt;/p&gt;

&lt;p&gt;That’s cool!  I swapped out the internals and I know to some degree that the external users of this unit will be OK.&lt;/p&gt;

&lt;p&gt;The new code that I swapped in, I had used before but did not have tests for it.  So I knew for example that my syntax for calling getopt was OK.&lt;br /&gt;
I did have trouble compiling, because I was using -Werror and you probably know with crazy casts and use of const and pointers and arrays, you get warnings.  In my case the compiler didn’t like my use of const char * argv[].&lt;br /&gt;
When I got the new code to compile, the above mentions tests FAILED !  Why???&lt;/p&gt;

&lt;p&gt;I figured it was because I changed something with the types and casts to make the compiler happy.&lt;/p&gt;

&lt;p&gt;I tried a bunch of things that didn’t make any difference.  Casting to (char **).&lt;/p&gt;

&lt;p&gt;I then found out, that, getopt uses global variables that do not get reset !!!&lt;br /&gt;
Now, obviously there were global variables because you have to use them, &lt;strong&gt;optarg&lt;/strong&gt;.&lt;br /&gt;
But to me I had no idea there was another global variable,&lt;br /&gt;
&lt;strong&gt;optind&lt;/strong&gt; ,&lt;br /&gt;
and this is the argument index!  Starts at the beginning of the command line and when it gets to the end, that’s what makes the next getopt() call return EOF!&lt;/p&gt;

&lt;p&gt;In my tests, getopt() was being called in each test!  Of course only the first test worked!&lt;/p&gt;

&lt;p&gt;So obviously the solution is to put the following line in the TEST_START{} block or whatever you call it, it’s the code that runs before every test!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;optind = 0&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I literally added that one line and all my tests passed.  Crazy!&lt;/p&gt;</content><author><name></name></author><summary type="html">I have a function with associated unit tests, called Parse_Bootloader_Commandline(). int Parse_Bootloader_Commandline(int argc, char * const argv[]) It clearly takes in the argc and argv from main(), and parses it. I had it working, and then I wanted to change the internals of that function, to use getopt. Interesting, I found that the unit tests were still applicable. They had names eg. ParseCommandline_InvalidCommand_Returns0 ParseCommandline_ValidLoadCommand_SetsCommandVariable That’s cool! I swapped out the internals and I know to some degree that the external users of this unit will be OK.</summary></entry></feed>