<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2017-10-06T09:46:17-07:00</updated><id>/</id><title type="html">Bootladder News</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</subtitle><entry><title type="html">Reverse SSH access to beaglebone</title><link href="/2017/10/05/reverse-ssh-access-to-beaglebone.html" rel="alternate" type="text/html" title="Reverse SSH access to beaglebone" /><published>2017-10-05T00:00:00-07:00</published><updated>2017-10-05T00:00:00-07:00</updated><id>/2017/10/05/reverse-ssh-access-to-beaglebone</id><content type="html" xml:base="/2017/10/05/reverse-ssh-access-to-beaglebone.html">&lt;p&gt;I’m leaving for the weekend and I want access to my 2 beaglebones.&lt;br /&gt;
1 is my system under test, the other controls a USB programmer
that can flash a microcontroller on the SUT.&lt;/p&gt;

&lt;p&gt;Following this guide http://xmodulo.com/access-linux-server-behind-nat-reverse-ssh-tunnel.html&lt;/p&gt;

&lt;p&gt;First I’ll locally SSH into the beaglebone.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;homeserver~$ ssh -fN -R 10022:localhost:22 relayserver_user@1.1.1.1 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;-R for reverse tunnel.  Port 10022 on remote host is forwarded to port 22 on beaglebone&lt;/li&gt;
  &lt;li&gt;10022 is arbitrary&lt;/li&gt;
  &lt;li&gt;port 22 is the SSH port that beaglebone sshd is listening on&lt;/li&gt;
  &lt;li&gt;-f for background&lt;/li&gt;
  &lt;li&gt;-N for “don’t execute a command”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:  I forgot that my VPS had disabled password logins; only permitted by key.&lt;br /&gt;
I had only installed 1 key on the VPS:  for my main laptop.&lt;br /&gt;
So to install 2 more keys, I SSHed into the VPS with my main laptop.&lt;br /&gt;
Temporarily allowed password logins.&lt;br /&gt;
Then used ssh-copy-id to install the keys on VPS.&lt;br /&gt;
Finally, disable password logins again.&lt;/p&gt;

&lt;p&gt;Last step is to add the command in rc.local.&lt;/p&gt;

&lt;h1 id=&quot;couple-extra-details&quot;&gt;Couple extra details&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;One of the beaglebones was running SSH on port 6000.  So, I had to change the 22 to a 6000.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;homeserver~$ ssh -fN -R 10022:localhost:6000 relayserver_user@1.1.1.1 
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;One of the beaglebones was only accepting SSH logins by key, but it only accepted one of a static set of keys.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was annoying, I had to scp the key over to the VPS.  Then to login to the beaglebone thru the tunnel,
I had to supply the key with ssh -i.&lt;/p&gt;

&lt;h1 id=&quot;important-detail-for-having-2-tunnels&quot;&gt;Important detail for having 2+ tunnels&lt;/h1&gt;

&lt;p&gt;The port 10022 is arbitrary but if there are 2 tunnels they can’t be both on 10022.&lt;br /&gt;
So actually one of them was on 10023.&lt;br /&gt;
When SSHing through the tunnel I specified which tunnel by specifying the port.&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m leaving for the weekend and I want access to my 2 beaglebones. 1 is my system under test, the other controls a USB programmer that can flash a microcontroller on the SUT. Following this guide http://xmodulo.com/access-linux-server-behind-nat-reverse-ssh-tunnel.html First I’ll locally SSH into the beaglebone.</summary></entry><entry><title type="html">Keep my hosts running</title><link href="/2017/10/04/keep-my-hosts-running.html" rel="alternate" type="text/html" title="Keep my hosts running" /><published>2017-10-04T00:00:00-07:00</published><updated>2017-10-04T00:00:00-07:00</updated><id>/2017/10/04/keep-my-hosts-running</id><content type="html" xml:base="/2017/10/04/keep-my-hosts-running.html">&lt;p&gt;Now that I temporarily host my friend’s site who actually wants to have visitors,
I can’t let it go down.  Also this blog, might as well keep it up.&lt;/p&gt;

&lt;p&gt;So first, when the VPS host goes down and reboots my VPS, my VPS has to start up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reverse Proxy, blog, friend’s site&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;systemd service looks OK but… feel safer with a shell script.&lt;/p&gt;

&lt;p&gt;To get this going, I’ll reboot the VPS and repeat the steps.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;service docker start;
cd /opt/nginx-proxy/ ; docker-compose up &amp;amp;
cd /opt/deploy/ ; docker-compose up &amp;amp;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;stick it in rc.local&lt;/p&gt;

&lt;p&gt;nice!&lt;/p&gt;

&lt;p&gt;Now how about a way to check if the website ever goes down?&lt;br /&gt;
Well if the host goes down it can’t tell me about it.&lt;br /&gt;
So, it could be the stopping of keepalive messages out of the host,
or a different host being unable to connect to the host.&lt;/p&gt;

&lt;p&gt;eh, leave that one at that for now.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;l&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Now that I temporarily host my friend’s site who actually wants to have visitors, I can’t let it go down. Also this blog, might as well keep it up. So first, when the VPS host goes down and reboots my VPS, my VPS has to start up.</summary></entry><entry><title type="html">beaglebone MCU programmer unit</title><link href="/2017/10/04/beaglebone-mcu-programmer-unit.html" rel="alternate" type="text/html" title="beaglebone MCU programmer unit" /><published>2017-10-04T00:00:00-07:00</published><updated>2017-10-04T00:00:00-07:00</updated><id>/2017/10/04/beaglebone-mcu-programmer-unit</id><content type="html" xml:base="/2017/10/04/beaglebone-mcu-programmer-unit.html">&lt;p&gt;I’m flashing Atmel ARM microcontrollers with a USB programmer dongle.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/bootladder/edbg
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Got a fresh beaglebone running, grabbed the IP from the local router.&lt;br /&gt;
In order to get the git clone to work I had to get the time right.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install ntp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;the compiled edbg binary is not included so it must be installed.&lt;br /&gt;
Let’s write the install script here.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp libudev.h /usr/include;
cp libudev.so /lib ;
make;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hmm looks like the library I included in there is out of date.  libc perhaps?&lt;/p&gt;

&lt;p&gt;Fortunately it worked after a&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libudev-dev 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So I’ll just take that copy of libudev.so out of the repo.&lt;/p&gt;

&lt;p&gt;So now it looks like this&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt-get install libudev-dev
make;
cp edbg /bin/;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then I run a&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@beaglebone:/tmp# edbg -l
Attached debuggers:
  J41800059345 - Atmel Corp. Atmel-ICE CMSIS-DAP
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Sweet!&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m flashing Atmel ARM microcontrollers with a USB programmer dongle. https://github.com/bootladder/edbg Got a fresh beaglebone running, grabbed the IP from the local router. In order to get the git clone to work I had to get the time right. apt-get install ntp the compiled edbg binary is not included so it must be installed. Let’s write the install script here. cp libudev.h /usr/include; cp libudev.so /lib ; make; Hmm looks like the library I included in there is out of date. libc perhaps? Fortunately it worked after a apt-get install libudev-dev So I’ll just take that copy of libudev.so out of the repo. So now it looks like this apt-get install libudev-dev make; cp edbg /bin/; Then I run a root@beaglebone:/tmp# edbg -l Attached debuggers: J41800059345 - Atmel Corp. Atmel-ICE CMSIS-DAP Sweet!</summary></entry><entry><title type="html">Use Phone WiFi Hotspot as default gateway for LAN</title><link href="/2017/09/18/use-phone-wifi-hotspot-as-default-gateway-for-lan.html" rel="alternate" type="text/html" title="Use Phone WiFi Hotspot as default gateway for LAN" /><published>2017-09-18T00:00:00-07:00</published><updated>2017-09-18T00:00:00-07:00</updated><id>/2017/09/18/use-phone-wifi-hotspot-as-default-gateway-for-lan</id><content type="html" xml:base="/2017/09/18/use-phone-wifi-hotspot-as-default-gateway-for-lan.html">&lt;p&gt;Big telecom companies are garbage.  Turd sandwich or giant douche?&lt;br /&gt;
Comcast: 1 year contract for $100/mo, 50Mb down.  No thanks!&lt;br /&gt;
Let’s look into mobile internet with unlimited data.  Let’s see if I can do this with my phone.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First I grab an extra laptop that has both Ethernet and WiFi.  This is the gateway&lt;/li&gt;
  &lt;li&gt;I turn on my phone’s hotspot and connect to it with the gateway laptop&lt;/li&gt;
  &lt;li&gt;Verify I can ping the gateway laptop from my tester client&lt;/li&gt;
  &lt;li&gt;Change default gateway on tester client to 10.0.0.6, verify Internet is now unavailable&lt;/li&gt;
  &lt;li&gt;On the gateway laptop, run this&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo sysctl -w net.ipv4.ip_forward=1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now Internet is available through gateway laptop, but the gateway now has to go out the WiFi hotspot.&lt;/p&gt;

&lt;p&gt;Turns out there are 2 0.0.0.0 entries in the route table.  Let’s delete the one for Ethernet and keep the WiFi one.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;route del default gateway x.x.x.x
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hmm, ping doesn’t work.  Just hangs, no ouptut.  I see what’s happening using tcpdump … traffic starts at the tester client, gets to the gateway on the Ethernet interface, then goes out the WiFi interface, but the source address is still the tester client’s source.&lt;br /&gt;
If I run a ping on the gateway laptop, it does go through.  The obvious difference is the source address.  Let’s change that.&lt;/p&gt;

&lt;p&gt;Let’s try this one&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# iptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j SNAT --to-source 1.2.3.4
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;wow-that-made-it-work&quot;&gt;Wow, that made it work!&lt;/h1&gt;</content><author><name></name></author><summary type="html">Big telecom companies are garbage. Turd sandwich or giant douche? Comcast: 1 year contract for $100/mo, 50Mb down. No thanks! Let’s look into mobile internet with unlimited data. Let’s see if I can do this with my phone. First I grab an extra laptop that has both Ethernet and WiFi. This is the gateway I turn on my phone’s hotspot and connect to it with the gateway laptop Verify I can ping the gateway laptop from my tester client Change default gateway on tester client to 10.0.0.6, verify Internet is now unavailable On the gateway laptop, run this sudo sysctl -w net.ipv4.ip_forward=1 Now Internet is available through gateway laptop, but the gateway now has to go out the WiFi hotspot. Turns out there are 2 0.0.0.0 entries in the route table. Let’s delete the one for Ethernet and keep the WiFi one. route del default gateway x.x.x.x Hmm, ping doesn’t work. Just hangs, no ouptut. I see what’s happening using tcpdump … traffic starts at the tester client, gets to the gateway on the Ethernet interface, then goes out the WiFi interface, but the source address is still the tester client’s source. If I run a ping on the gateway laptop, it does go through. The obvious difference is the source address. Let’s change that. Let’s try this one # iptables -t nat -A POSTROUTING ! -d 192.168.0.0/16 -o eth1 -j SNAT --to-source 1.2.3.4 Wow, that made it work!</summary></entry><entry><title type="html">Making my version control not suck</title><link href="/2017/09/13/making-my-version-control-not-suck.html" rel="alternate" type="text/html" title="Making my version control not suck" /><published>2017-09-13T00:00:00-07:00</published><updated>2017-09-13T00:00:00-07:00</updated><id>/2017/09/13/making-my-version-control-not-suck</id><content type="html" xml:base="/2017/09/13/making-my-version-control-not-suck.html">&lt;p&gt;I develop and maintain firmware for a family of products.
There are 5 different firmware projects here.
They’re all releated in communication protocol, as well as other things
such as beep frequency, LED blink patterns etc.&lt;/p&gt;

&lt;p&gt;But now I’m faced with splitting all 5 of those into 3 versions.
So now there are technically 15 different firmware images that need to be
available for download and documented to identify what they are.&lt;/p&gt;

&lt;p&gt;My (git) version control system until now was obvious.  Master goes forward, occasional branching and merging, tags on version releases.&lt;/p&gt;

&lt;p&gt;It would be possible to use branches and tags to rebuild any version of any project.  But here’s the complication:  there is a in-house shared library used by all 15 firmware versions.  That library also has 3 versions.  In other words the correct version of the shared library has to be available when building a version of firmware.
  In more other words, to build a version of firmware, the project repo has to be checked out at the right version, as well as the shared library checked out at the right version.  Finally, the build config (eg. Makefile) has to identify and locate the correct shared library to be linked.&lt;/p&gt;

&lt;p&gt;I like stuff like Ruby, Rust, etc. where you have a config file (yaml) to specify dependencies.&lt;/p&gt;

&lt;p&gt;I think I can improve my Makefile to do what I want.&lt;br /&gt;
It would be as simple as git checkout my_commit followed by make.
The Makefile at this commit should build the proper version.  The Makefile itself identifies its own version and the dependency versions it needs.  It can then git clone the dependencies using git submodules.&lt;/p&gt;

&lt;p&gt;This way, a complete build with all dependencies lives inside 1 directory.  If a submodule is updated or branched, it won’t affect previous builds.&lt;/p&gt;

&lt;h1 id=&quot;strategy--use-a-fresh-machine-vps-to-recreate-the-build&quot;&gt;Strategy:  use a fresh machine (VPS) to recreate the build&lt;/h1&gt;

&lt;p&gt;I don’t want to be confused with currently existing directory structure.  Building the same firmware version on a different machine is a good way to shake out things like hard coded paths breaking.&lt;br /&gt;
Ideally all builds would happen on this separate machine.  I guess that’s part of Continous Integration?&lt;/p&gt;

&lt;h1 id=&quot;-makefile-needs-source-located-in-a-hard-coded-directory&quot;&gt;— Makefile needs source located in a hard coded directory&lt;/h1&gt;

&lt;p&gt;Project needs a library or some source somewhere else to compile.
The Makefile can find the source with a VPATH and find the headers
with -I compiler flags.  But, that enforces a certain directory structure.&lt;/p&gt;

&lt;h1 id=&quot;-new-rule-discovery-only-create-directory-structure-for-sub-directories--never-depend-on-paths-starting-wtih--or--etc&quot;&gt;— New rule discovery?: Only create directory structure for sub-directories.  Never depend on paths starting wtih ../ or ../../ etc.&lt;/h1&gt;

&lt;p&gt;This rule allows you to create directory structure, but doesn’t cause problems because the structure is all below current directory.&lt;br /&gt;
Basically, enforced sub-directory structure is coupled entirely to
the populating of that sub-directory and makes no assumptions about directories above current directory.&lt;br /&gt;
In other words, if you have enforced structure, the build system has to automatically conform to that structure.  Let the Makefile create the directories with their correct names, populate them with files.&lt;/p&gt;

&lt;h1 id=&quot;-vpath-issue-with-git-clone&quot;&gt;— VPATH Issue with git clone&lt;/h1&gt;

&lt;p&gt;I have a repo prereq inside my all target.
The purpose is to clone the dependencies,
so Make can then refer to those dependencies and build the target.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;always: 
repos: always
        @echo hello
        if [ ! -d &quot;../../mysrcrepo&quot; ] ; then \
            git clone https://mysrcrepo ;\
        fi 
        if [ ! -d &quot;../../mylibrepo&quot; ] ; then \
            git clone https://mylibrepo ;\
            make; \

        fi 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It almost worked.
  In the case of a cloned library, the make command is run, which builds the library.  That worked.
  But in the case of cloned source, the VPATH didn’t catch the source files that were cloned.  When running make a second time, it would work.&lt;/p&gt;

&lt;p&gt;Well apparently 2 things are wrong with that, as I’ve read.&lt;br /&gt;
1, VPATH doesn’t work on generated files.  Not sure what that means specifically but the way I made sense of it is this:  You can add directories to VPATH but if they don’t exist when make is run, the VPATH won’t see that directory.  Even if the file exists at the time it is needed, it won’t be found.  That explains why the build works the second time.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Recursive Make Considered Harmful?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;discovery--how-to-do-git-inside-make&quot;&gt;Discovery:  how to do git inside make&lt;/h1&gt;

&lt;p&gt;https://stackoverflow.com/questions/15602059/git-shortcut-to-pull-with-clone-if-no-local-there-yet&lt;/p&gt;

&lt;p&gt;Using this I can solve the problem in the above snippet
        if [ ! -d “../../mysrcrepo” ] ; then \&lt;/p&gt;

&lt;p&gt;If the directory does exist, nothing happens.  What I want is a git pull in that case.&lt;/p&gt;</content><author><name></name></author><summary type="html">I develop and maintain firmware for a family of products. There are 5 different firmware projects here. They’re all releated in communication protocol, as well as other things such as beep frequency, LED blink patterns etc.</summary></entry><entry><title type="html">super fast static website migration</title><link href="/2017/09/08/super-fast-static-website-migration.html" rel="alternate" type="text/html" title="super fast static website migration" /><published>2017-09-08T00:00:00-07:00</published><updated>2017-09-08T00:00:00-07:00</updated><id>/2017/09/08/super-fast-static-website-migration</id><content type="html" xml:base="/2017/09/08/super-fast-static-website-migration.html">&lt;p&gt;A friend was paying a company a crazy amount of money per month to create a static site,
and host it.&lt;br /&gt;
Friend realized they were being ripped off and stopped paying.
The company then shutdown the website host, blocking access to the site source.&lt;/p&gt;

&lt;p&gt;Fortunately there was a copy on the wayback machine.&lt;/p&gt;

&lt;p&gt;I ripped the last good copy and dumped it onto my own host.&lt;/p&gt;

&lt;p&gt;It was super easy and fast, took just a few hours to figure out a couple things.&lt;/p&gt;

&lt;p&gt;The entire website was roughly 1MB.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Actually you don’t even need a Dockerfile, you can work straight from this image
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  from sebp/lighttpd
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;docker-compose.yaml
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  version: '2'
  services:
    lighttpd:
      build: .
      environment:
        - VIRTUAL_HOST=mywebsite.com
      ports:
        - &quot;14001:80&quot;
      volumes:
        - ./website/:/var/www/localhost/htdocs

  networks:
    default:
      external:
        name: nginx-proxy
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;explanation&quot;&gt;Explanation:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;the environment variable VIRTUAL_HOST is for the nginx reverse proxy.&lt;br /&gt;
It’s name based so when you HTTP request the reverse proxy looking for hostname
mywebsite.com , the reverse proxy directs traffic to this container we’re discussing.&lt;/li&gt;
  &lt;li&gt;ports: the container lighttpd serves on port 80.  14001 is a random number.&lt;/li&gt;
  &lt;li&gt;volumes: ./website , ie. the current directory where the docker-compose.yaml file is,
is mapped to the web root of the lighttpd server.
Just stick the website in there.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;to-grab-the-website-from-wayback-machine-i-used-wayback-machine-downloader&quot;&gt;To grab the website from Wayback Machine, I used Wayback Machine Downloader&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/hartator/wayback-machine-downloader&quot;&gt;https://github.com/hartator/wayback-machine-downloader&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Was super easy to use, just took a minute to figure out the –to and –from flags.&lt;/li&gt;
  &lt;li&gt;When the site you’re interested in is currently pointing to a domain parking spot,
you want to get files up to a certain date.&lt;/li&gt;
  &lt;li&gt;If you want to exclude older files you’ll have to do that explicitly also.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">A friend was paying a company a crazy amount of money per month to create a static site, and host it. Friend realized they were being ripped off and stopped paying. The company then shutdown the website host, blocking access to the site source. Fortunately there was a copy on the wayback machine. I ripped the last good copy and dumped it onto my own host. It was super easy and fast, took just a few hours to figure out a couple things. The entire website was roughly 1MB.</summary></entry><entry><title type="html">Deploy blog updates automatically with git push</title><link href="/guide/2017/08/30/automatic-deploy-this-blog.html" rel="alternate" type="text/html" title="Deploy blog updates automatically with git push" /><published>2017-08-30T00:00:00-07:00</published><updated>2017-08-30T00:00:00-07:00</updated><id>/guide/2017/08/30/automatic-deploy-this-blog</id><content type="html" xml:base="/guide/2017/08/30/automatic-deploy-this-blog.html">&lt;p&gt;After I had the docker shared volume + git solution working (see other post),
I immediately noticed there were too many steps to work on the blog.&lt;/p&gt;

&lt;p&gt;Change blog source –&amp;gt; jekyll build –&amp;gt; git add &amp;amp;&amp;amp; git commit &amp;amp;&amp;amp; git push –&amp;gt; git pull&lt;/p&gt;

&lt;p&gt;The worst part being that git pull happens on the production host and the others happen on dev host.&lt;/p&gt;

&lt;p&gt;So, I found a bunch of posts talking about a push-to-deploy solution using git –bare.
It works by pushing commits to a bare repository located on production host.
The bare repo then has a post receive hook which installs files somewhere else on the production filesytem.&lt;/p&gt;

&lt;p&gt;I want to skip a step there, because fortunately, the Docker setup for this blog
maps the web server root directory to inside the git repo.  So no separate install directory needed.&lt;/p&gt;

&lt;p&gt;This guide is relevant, actually obvious but it’s nice to see it in a guide.  https://gist.github.com/joahking/780877&lt;/p&gt;

&lt;p&gt;Trying it out now…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stop the running Docker container&lt;/li&gt;
  &lt;li&gt;Go into a new directory so we don’t touch the working install&lt;/li&gt;
  &lt;li&gt;git clone –bare https://myrepo&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I get a blog.git directory, no source.
Ahh, now I get it.  There’s no working tree, there’s only objects (blobs).  The raw git stuff.
A working tree at any commit can be constructed using the objects and the history (did I say that right?).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;So, I can’t just have a bare repo.  I can’t have a non-bare repo because I can’t push to it.&lt;/li&gt;
  &lt;li&gt;So I guess the other guys were right.&lt;br /&gt;
Push to the bare repo and use the post receive hook
to create a working copy and install that.&lt;/li&gt;
  &lt;li&gt;But that doesn’t work if my Docker container is already running&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If Docker container is running, that means docker-compose up happened inside a git repo. (in my case)
If there was a separate bare repo, its post receive hook would install files inside another git repo.
Which is not too bad since I won’t ever push from that repo.
Now consider if the Dockerfile changes.  The container has to be rebuilt and restarted.
Pulling into the production non-bare repo might be awkward since its data is behind master in the history.
The post receive hook could copy the Dockerfile.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This may be more complicated than I like.  But perhaps the proper solution is best.
On production host we have the bare repo.&lt;br /&gt;
On dev we push to production –&amp;gt;  bare repo executes post receive
That post receive will install the Dockerfile, docker-compose.yaml, and the static website
into another location.&lt;br /&gt;
Actually this is nice since docker-compose.yaml uses relative paths “./” when mapping the
volume for http server root.&lt;br /&gt;
So we can actually deploy the website in any directory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Nice! It worked as expected.&lt;/li&gt;
  &lt;li&gt;git remote add prod&lt;/li&gt;
  &lt;li&gt;git push prod master&lt;/li&gt;
  &lt;li&gt;the bare prod repo executes its script and files appear inside the deploy directory!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I used the post-receive script from here
https://gist.github.com/thomasfr/9691385&lt;/p&gt;

&lt;p&gt;Works great.&lt;/p&gt;

&lt;p&gt;Actually you know what’s really cool, I don’t even need github.com to do this!  It’s just my laptop and prod.&lt;/p&gt;

&lt;p&gt;I could actually automate this further and combine:  jekyll build –&amp;gt; git add . –&amp;gt; git commit -m “insert message here” –&amp;gt; git push 
Into 1 command.&lt;/p&gt;

&lt;h1 id=&quot;now-i-can-deploy-changes-to-my-blog-with-1-git-push--as-demonstrated-by-this-particular-change&quot;&gt;Now I can deploy changes to my blog with 1 git push!  As demonstrated by this particular change!&lt;/h1&gt;</content><author><name></name></author><summary type="html">After I had the docker shared volume + git solution working (see other post), I immediately noticed there were too many steps to work on the blog. Change blog source –&amp;gt; jekyll build –&amp;gt; git add &amp;amp;&amp;amp; git commit &amp;amp;&amp;amp; git push –&amp;gt; git pull The worst part being that git pull happens on the production host and the others happen on dev host. So, I found a bunch of posts talking about a push-to-deploy solution using git –bare. It works by pushing commits to a bare repository located on production host. The bare repo then has a post receive hook which installs files somewhere else on the production filesytem. I want to skip a step there, because fortunately, the Docker setup for this blog maps the web server root directory to inside the git repo. So no separate install directory needed. This guide is relevant, actually obvious but it’s nice to see it in a guide. https://gist.github.com/joahking/780877 Trying it out now… Stop the running Docker container Go into a new directory so we don’t touch the working install git clone –bare https://myrepo I get a blog.git directory, no source. Ahh, now I get it. There’s no working tree, there’s only objects (blobs). The raw git stuff. A working tree at any commit can be constructed using the objects and the history (did I say that right?). So, I can’t just have a bare repo. I can’t have a non-bare repo because I can’t push to it. So I guess the other guys were right. Push to the bare repo and use the post receive hook to create a working copy and install that. But that doesn’t work if my Docker container is already running If Docker container is running, that means docker-compose up happened inside a git repo. (in my case) If there was a separate bare repo, its post receive hook would install files inside another git repo. Which is not too bad since I won’t ever push from that repo. Now consider if the Dockerfile changes. The container has to be rebuilt and restarted. Pulling into the production non-bare repo might be awkward since its data is behind master in the history. The post receive hook could copy the Dockerfile. This may be more complicated than I like. But perhaps the proper solution is best. On production host we have the bare repo. On dev we push to production –&amp;gt; bare repo executes post receive That post receive will install the Dockerfile, docker-compose.yaml, and the static website into another location. Actually this is nice since docker-compose.yaml uses relative paths “./” when mapping the volume for http server root. So we can actually deploy the website in any directory. Nice! It worked as expected. git remote add prod git push prod master the bare prod repo executes its script and files appear inside the deploy directory!</summary></entry><entry><title type="html">Hosting on docker behind a reverse proxy</title><link href="/guide/2017/08/29/docker-reverse-proxy-hosting.html" rel="alternate" type="text/html" title="Hosting on docker behind a reverse proxy" /><published>2017-08-29T00:00:00-07:00</published><updated>2017-08-29T00:00:00-07:00</updated><id>/guide/2017/08/29/docker-reverse-proxy-hosting</id><content type="html" xml:base="/guide/2017/08/29/docker-reverse-proxy-hosting.html">&lt;p&gt;Needed to redo my personal infrastructure.  My infrastructure at work absolutely needs work
but that’s out of my control.  But my personal stuff can complement my work stuff.&lt;/p&gt;

&lt;p&gt;Currently thinking about integrating:  blog, wiki, VPN, storage, CI server, etc.
And I would like some resources, particularly storage to be hostable on physical hardware in a home/office LAN.&lt;/p&gt;

&lt;p&gt;And in some way that isn’t a pile of mess and I can actually see what I have and what’s happening.&lt;/p&gt;

&lt;p&gt;First thing I did was point all my DNS at a reverse proxy server.&lt;br /&gt;
I wanted the RPS to run in a docker container.&lt;br /&gt;
First attempted to install docker on my $12/year VPS.  Turns out it was a OpenVZ VPS which did not have the minimum kernel 3.10 for docker.&lt;br /&gt;
So had to get a $15/year KVM VPS.  Cheap but only 5G disk.  Makes sense to be primarly a proxy and possibly the VPN server; not sure about VPN speed on a VPS…&lt;/p&gt;

&lt;p&gt;Used jwilder/nginx-proxy on docker hub.
Works great, also can add your own config for a host.  For example I want some hosts behind the proxy to be external to the proxy machine, ie. not running in a container inside the proxy machine.&lt;/p&gt;

&lt;p&gt;Then I wanted to host this blog behind the proxy.
lighttpd install.  This particular one has a VOLUME of its data directory.
So I just use docker-compose.yaml to map the volume to where the static site is stored.&lt;/p&gt;

&lt;p&gt;I tried putting my wiki behind the proxy but was getting weird behavior most probably related to URLs or HTTP headers.&lt;br /&gt;
Actually it mostly worked but logging in made it act weird.&lt;/p&gt;

&lt;p&gt;Now that I could host the site in a docker container and have it behind the proxy,
I had to figure how to deploy changes.&lt;br /&gt;
Found a solution, though it has coupling not sure how to avoid… chef/puppet?&lt;/p&gt;

&lt;p&gt;I create a git repository containing:
the static site source code, and the built site.
a docker-compose.yaml and Dockerfile&lt;/p&gt;

&lt;p&gt;to develop I have a laptop that can build jekyll, so I pull the repo, edit/new files, 
jekyll build, push the repo which includes the built site.&lt;/p&gt;

&lt;p&gt;Then on my production host I run git pull to pull the built site.
Then docker-compose up serves the site.&lt;/p&gt;

&lt;p&gt;I don’t even have to touch docker!  I just go to production host where the repo is, git pull, and since the volume is mapped, the changes show immediately.&lt;/p&gt;

&lt;p&gt;There is coupling to the reverse proxy; a docker network.&lt;/p&gt;</content><author><name></name></author><summary type="html">Needed to redo my personal infrastructure. My infrastructure at work absolutely needs work but that’s out of my control. But my personal stuff can complement my work stuff.</summary></entry><entry><title type="html">How to test getopt</title><link href="/guide/2017/06/20/Unit-Testing-Legacy-C-Embedded.html" rel="alternate" type="text/html" title="How to test getopt" /><published>2017-06-20T00:00:00-07:00</published><updated>2017-06-20T00:00:00-07:00</updated><id>/guide/2017/06/20/Unit-Testing-Legacy-C-Embedded</id><content type="html" xml:base="/guide/2017/06/20/Unit-Testing-Legacy-C-Embedded.html">&lt;p&gt;I have a function with associated unit tests, called Parse_Bootloader_Commandline().
  int Parse_Bootloader_Commandline(int argc, char * const argv[])&lt;br /&gt;
It clearly takes in the argc and argv from main(), and parses it.&lt;/p&gt;

&lt;p&gt;I had it working, and then I wanted to change the internals of that function, to use getopt.&lt;br /&gt;
Interesting, I found that the unit tests were still applicable.  They had names eg.&lt;br /&gt;
ParseCommandline_InvalidCommand_Returns0&lt;br /&gt;
ParseCommandline_ValidLoadCommand_SetsCommandVariable&lt;/p&gt;

&lt;p&gt;That’s cool!  I swapped out the internals and I know to some degree that the external users of this unit will be OK.&lt;/p&gt;

&lt;p&gt;The new code that I swapped in, I had used before but did not have tests for it.  So I knew for example that my syntax for calling getopt was OK.&lt;br /&gt;
I did have trouble compiling, because I was using -Werror and you probably know with crazy casts and use of const and pointers and arrays, you get warnings.  In my case the compiler didn’t like my use of const char * argv[].&lt;br /&gt;
When I got the new code to compile, the above mentions tests FAILED !  Why???&lt;/p&gt;

&lt;p&gt;I figured it was because I changed something with the types and casts to make the compiler happy.&lt;/p&gt;

&lt;p&gt;I tried a bunch of things that didn’t make any difference.  Casting to (char **).&lt;/p&gt;

&lt;p&gt;I then found out, that, getopt uses global variables that do not get reset !!!&lt;br /&gt;
Now, obviously there were global variables because you have to use them, &lt;strong&gt;optarg&lt;/strong&gt;.&lt;br /&gt;
But to me I had no idea there was another global variable,&lt;br /&gt;
&lt;strong&gt;optind&lt;/strong&gt; ,&lt;br /&gt;
and this is the argument index!  Starts at the beginning of the command line and when it gets to the end, that’s what makes the next getopt() call return EOF!&lt;/p&gt;

&lt;p&gt;In my tests, getopt() was being called in each test!  Of course only the first test worked!&lt;/p&gt;

&lt;p&gt;So obviously the solution is to put the following line in the TEST_START{} block or whatever you call it, it’s the code that runs before every test!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;optind = 0&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I literally added that one line and all my tests passed.  Crazy!&lt;/p&gt;</content><author><name></name></author><summary type="html">I have a function with associated unit tests, called Parse_Bootloader_Commandline(). int Parse_Bootloader_Commandline(int argc, char * const argv[]) It clearly takes in the argc and argv from main(), and parses it. I had it working, and then I wanted to change the internals of that function, to use getopt. Interesting, I found that the unit tests were still applicable. They had names eg. ParseCommandline_InvalidCommand_Returns0 ParseCommandline_ValidLoadCommand_SetsCommandVariable That’s cool! I swapped out the internals and I know to some degree that the external users of this unit will be OK.</summary></entry><entry><title type="html">How to make Linux Makefiles Work Good</title><link href="/post/2017/05/22/How-to-Makefile-Linux-Good.html" rel="alternate" type="text/html" title="How to make Linux Makefiles Work Good" /><published>2017-05-22T00:00:00-07:00</published><updated>2017-05-22T00:00:00-07:00</updated><id>/post/2017/05/22/How-to-Makefile-Linux-Good</id><content type="html" xml:base="/post/2017/05/22/How-to-Makefile-Linux-Good.html">&lt;p&gt;I’m going to describe what I do for my current project.  I’m porting over my 5th project to this system so I gotta write it down haha.&lt;/p&gt;

&lt;p&gt;Main Elements:&lt;br /&gt;
VPATH:  You have to append every source directory in the VPATH.&lt;br /&gt;
OBJS:  Then you list all the objects being compiled&lt;/p&gt;

&lt;p&gt;Use a command like this:
OBJS_PRODUCTION_CROSS = $(addprefix build/,$(PRODUCTION_OBJ_NAMES) $(PRODUCTION_TARGET_SPECIFIC_OBJ_NAMES) $(PRODUCTION_APP_OBJ_NAMES) $(ASFNAMES) )&lt;/p&gt;

&lt;p&gt;To create a list of Target Object Paths.  These are the objects you want to build.  They’ll be in the /build directory.&lt;/p&gt;

&lt;p&gt;The Make targets:  For building each object use a Make target like this:
build/%.o : %.c
  @echo Building file: $&amp;lt;
  @echo Invoking: ARM/GNU C Compiler : 5.3.1
  $(CROSS_C_COMPILER)  -x c -mthumb $(PROJECT_VERSION_NUMBER)  $(PROJECT_FLAGS) $(PROJECT_INCLUDES) $(PROJECT_COMMON_FLAGS)  -mlong-calls -g3 -Wall -Werror -m&lt;/p&gt;

&lt;p&gt;This is why you made that list of the target objects before.  That’s a list of dependencies.  Every object is a dependency.&lt;br /&gt;
The Make target covers all of these.&lt;br /&gt;
The cool part is the dependency : %.c here.  This is where the VPATH is cool.
No need to specify where any particular source file is.  The VPATH contains all the directories with source.&lt;/p&gt;

&lt;p&gt;To build that source C file, all you need is the path to it so that’s all the VPATH does.&lt;br /&gt;
The rest is just obvious compiler flags!&lt;/p&gt;

&lt;p&gt;So what I”m doing is converting a Visual Studio generated makefile with all this Windows specific crap.&lt;br /&gt;
Into a PORTABLE-ABLE MAKEFILE THAT I HAPPEN TO BE USING WITH LINUX.  Ahem.&lt;/p&gt;

&lt;p&gt;Let’s create that PORTABLE-ABLE Makefile starter template, might as well.&lt;/p&gt;

&lt;p&gt;To get incremental progress I’ll start with the all target and go from there.&lt;/p&gt;

&lt;p&gt;First the all target:&lt;/p&gt;
&lt;h1 id=&quot;all-target&quot;&gt;All Target&lt;/h1&gt;
&lt;p&gt;all: $(OUTPUT_FILE_PATH) $(ADDITIONAL_DEPENDENCIES)&lt;/p&gt;

&lt;p&gt;$(OUTPUT_FILE_PATH): $(OBJS_PRODUCTION_CROSS) $(USER_OBJS) $(LIB_DEP) $(LINKER_SCRIPT_DEP)
  @echo Building target: $@
  @echo Invoking: ARM/GNU Linker : 5.3.1
  $(CROSS_C_COMPIL&lt;/p&gt;

&lt;p&gt;What we see here is all will make the target with name specified by variable $(OUTPUT_FILE_PATH)&lt;br /&gt;
To make that target, it depends on our production objects, libraries, a linker script, and whatever else.&lt;br /&gt;
Let’s get that working.&lt;/p&gt;

&lt;p&gt;#&lt;/p&gt;
&lt;h1 id=&quot;generals&quot;&gt;GENERALS&lt;/h1&gt;
&lt;p&gt;#
#
#&lt;/p&gt;

&lt;p&gt;$(TARGET_OBJECT_BUILD_DIRECTORY)/%.o : %.c
  @echo Building file: $&amp;lt;
  @echo Invoking: C Compiler 
  @echo $(COMPILER_COMMAND)
  $(COMPILER_COMMAND)&lt;/p&gt;

&lt;h1 id=&quot;all-target-1&quot;&gt;All Target&lt;/h1&gt;
&lt;p&gt;all: $(OUTPUT_FILE_PATH) $(ADDITIONAL_DEPENDENCIES)&lt;/p&gt;

&lt;p&gt;$(OUTPUT_FILE_PATH): $(DEPENDENCIES_BEFORE_LINKING)
  @echo Building target: $@&lt;/p&gt;

&lt;p&gt;Works!&lt;/p&gt;

&lt;p&gt;Now to use this, I need to create variables DEPENDENCIES_BEFORE_LINKING.
These are the objects and libraries and stuff I need, linker script etc.
If DEPENDENCIES_BEFORE_LINKING has the format described below, the provided make target works.&lt;/p&gt;

&lt;p&gt;When the list DEPENDENCIES_BEFORE_LINKING includes .o object files inside the TARGET_OBJECT_BUILD_DIRECTORY,
they will match the target.  The source file will be found from the VPATH.&lt;/p&gt;</content><author><name>Steven Anderson</name></author><summary type="html">I’m going to describe what I do for my current project. I’m porting over my 5th project to this system so I gotta write it down haha. Main Elements: VPATH: You have to append every source directory in the VPATH. OBJS: Then you list all the objects being compiled Use a command like this: OBJS_PRODUCTION_CROSS = $(addprefix build/,$(PRODUCTION_OBJ_NAMES) $(PRODUCTION_TARGET_SPECIFIC_OBJ_NAMES) $(PRODUCTION_APP_OBJ_NAMES) $(ASFNAMES) ) To create a list of Target Object Paths. These are the objects you want to build. They’ll be in the /build directory. The Make targets: For building each object use a Make target like this: build/%.o : %.c @echo Building file: $&amp;lt; @echo Invoking: ARM/GNU C Compiler : 5.3.1 $(CROSS_C_COMPILER) -x c -mthumb $(PROJECT_VERSION_NUMBER) $(PROJECT_FLAGS) $(PROJECT_INCLUDES) $(PROJECT_COMMON_FLAGS) -mlong-calls -g3 -Wall -Werror -m</summary></entry></feed>